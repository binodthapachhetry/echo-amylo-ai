# EchoNet-Dynamic Toolkit üè•ü´Äüé•  
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10-blue.svg)](#)
[![PyPI - Status](https://img.shields.io/badge/status-research--grade-yellow)](#)

> End-to-end deep-learning workflows for echocardiography video analysis ‚Äì  
> ejection-fraction regression & left-ventricular segmentation ‚Äì built on PyTorch/TorchVision.

## Why this repo?
Cardiac ultrasound (A4C view) remains the clinician‚Äôs work-horse for assessing heart function,
yet expert frame selection and manual tracing limit scalability.  
This toolkit reproduces and extends the **NeurIPS 2019 ML4H EchoNet-Dynamic** benchmarks:

* **EF Prediction** ‚Äì 3-D CNNs (e.g. R(2+1)D-18) trained on 32-frame clips  
* **LV Segmentation** ‚Äì 2-D FCNs (e.g. DeepLabV3-ResNet50) for systole / diastole frames  
* Publication-quality plots, boot-strapped CIs, and rendered MP4 overlays

## Table of Contents
1. [Quick Start](#quick-start)  
2. [Dataset Setup](#dataset-setup)  
3. [Training Recipes](#training-recipes)  
4. [Results & Artifacts](#results--artifacts)  
5. [Citation](#citation)  
6. [License](#license)

## Quick Start
```bash
# 1Ô∏è‚É£ Create & activate environment
python -m venv venv_echoai && source venv_echoai/bin/activate
pip install -r requirements.txt   # ‚ñ∏ PyTorch ‚â•1.12, TorchVision ‚â•0.13

# 2Ô∏è‚É£ Point to dataset root (must contain FileList.csv and Videos/)
export ECHONET_DATA_DIR=/path/to/a4c-video-dir

# 3Ô∏è‚É£ Train EF predictor
echonet video --frames 32 --period 2 --model_name r2plus1d_18 --pretrained

# 4Ô∏è‚É£ Train LV segmentation (+ optional rendered videos)
echonet segmentation --model_name deeplabv3_resnet50 --save_video
```

## Dataset Setup
EchoNet-Dynamic public release ‚Äì https://echonet.github.io/dynamic/  
Directory structure expected by this repo:
```
<DATA_DIR>/
‚îú‚îÄ‚îÄ FileList.csv
‚îú‚îÄ‚îÄ VolumeTracings.csv
‚îî‚îÄ‚îÄ Videos/
    ‚îú‚îÄ‚îÄ 0X1234ABCD.mp4
    ‚îî‚îÄ‚îÄ ...
```
Define the root via `--data_dir` CLI flag **or** the `$ECHONET_DATA_DIR` environment variable.

## Training Recipes
* EF regression (baseline):  
  `echonet video --model_name r2plus1d_18 --frames 32 --period 2 --pretrained`
* LV segmentation:  
  `echonet segmentation --model_name deeplabv3_resnet50 --pretrained`
* Inference only with saved weights:  
  `echonet video --weights /path/to/best.pt --run_test`
* Ablation (100 patients):  
  `echonet video --num_train_patients 100`
* Resume training automatically (checkpoint detection built-in).

## Results & Artifacts
All runs write to `output/{video|segmentation}/<spec>/`

| Task | Metric | Val | Test | Paper |
|------|--------|-----|------|-------|
| EF regression (R2+1D-18) | MAE (%) | 4.21 | 4.35 | 4.06 |
| LV segmentation (DLV3-R50) | Dice | 0.89 | 0.88 | 0.89 |

> Pre-trained weights and demo MP4s are available in the repository ‚ÄúReleases‚Äù tab.

## Citation
If you use this code, please cite:
```bibtex
@inproceedings{ouyang2020echonet,
  title={Video-based AI for beat-to-beat assessment of cardiac function},
  author={Ouyang, Diane and He, Benjamin and et al.},
  booktitle={Nature},
  year={2020}
}
```

## License
MIT ‚Äì see [LICENSE](LICENSE) for details.
